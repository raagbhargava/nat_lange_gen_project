{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c100ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 01:11:39.425838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-14 01:11:39.425940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-14 01:11:39.571544: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-14 01:11:39.849017: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d79cd8908dc496a8f9c253ea18bd201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781ff2fba8054e0996953566bd98fbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874155657d7345bf8cf777e6a0048a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a91ff202554309875c5c26be21826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764c50f20d7b4b0997a4f7e924a34b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d23a9d76bd4a0bac8c3b8e49973fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0364b2dd7e954af7936820b03f140e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, GPTNeoModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pronouncing\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Download Finetuned GPT-Neo\n",
    "# Set the random seed to a fixed value to get reproducible results \n",
    "torch.manual_seed(42)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", \n",
    "                                          bos_token=\"<|startoftext|>\",\n",
    "                            eos_token=\"<|endoftext|>\",\n",
    "                            pad_token=\"<|pad|>\")\n",
    "\n",
    "# Download the pre-trained GPT-Neo model and transfer it to the GPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"FigoMe/news-gpt-neo-1.3B-keywords-line-by-line-reverse\").cuda()\n",
    "# Resize the token embeddings because we've just added 3 new tokens \n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def get_stress(phone):\n",
    "    stress = []\n",
    "    for s in phone.split():\n",
    "        if s[-1].isdigit():\n",
    "            if s[-1] == '2':\n",
    "                stress.append(0)\n",
    "            else:\n",
    "                stress.append(int(s[-1]))\n",
    "    return stress\n",
    "\n",
    "def alternating(stress):\n",
    "    #Check if the stress and unstress are alternating\n",
    "    check1 = len(set(stress[::2])) <= 1 and (len(set(stress[1::2])) <= 1)\n",
    "    check2 = len(set(stress)) == 2 if len(stress) >=2 else True\n",
    "    return (check1 and check2)\n",
    "\n",
    "def get_phones(rhyme_word):\n",
    "    phone = pronouncing.phones_for_word(rhyme_word)[0]\n",
    "    stress = get_stress(phone)\n",
    "    p_state = stress[0]\n",
    "    n_syllables = len(stress)\n",
    "    return p_state, n_syllables\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(\n",
    "    logits: Tensor,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 1.0,\n",
    "    filter_value: float = -float(\"Inf\"),\n",
    "    min_tokens_to_keep: int = 1,\n",
    "    return_index = False\n",
    ") -> Tensor:\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (batch size, vocabulary size)\n",
    "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = logits >= torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        indices_keep = indices_keep[0].tolist()\n",
    "        indices_keep = [i for i,x in enumerate(indices_keep) if x == True]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    if return_index == True:\n",
    "        return logits, indices_keep\n",
    "    return logits\n",
    "\n",
    "\n",
    "def reverse_order(line):\n",
    "    line = line.replace(', ', ' , ')\n",
    "    words = line.split()\n",
    "    return ' '.join(reversed(words)).replace(' , ', ', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6397af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loose_list = ['that','is','of','the','it','a','as','with','like','go','to','on','in','at','are','and']\n",
    "def check_either_stress(stress, source_word, loose = True):\n",
    "    if loose and source_word in loose_list:\n",
    "        return True\n",
    "    if len(stress) == 1 and len(pronouncing.phones_for_word(source_word))>1:\n",
    "                    phone0 = pronouncing.phones_for_word(source_word)[0]\n",
    "                    phone1 = pronouncing.phones_for_word(source_word)[1]\n",
    "                    stress0 = [int(s[-1]) for s in phone0.split() if s[-1].isdigit()]\n",
    "                    stress1 = [int(s[-1]) for s in phone1.split() if s[-1].isdigit()]\n",
    "                    if stress0+stress1 ==1 and stress0*stress1 == 0:\n",
    "                        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e301bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(input_ids1, temperature = 0.85, topk = 100, n_sample=10, device = 'cuda:0'):\n",
    "    current_word = 0\n",
    "    original = tokenizer.decode(input_ids1[0])\n",
    "    for _ in range(1):\n",
    "        outputs1 = model(input_ids1)\n",
    "        #print(outputs1)\n",
    "        next_token_logits1 = outputs1[0][:, -1, :]\n",
    "        next_token_logits1 = top_k_top_p_filtering(next_token_logits1, top_k=topk)\n",
    "        logit_zeros = torch.zeros(len(next_token_logits1)).cuda()\n",
    "        #logit_zeros = torch.zeros(len(next_token_logits1), device=device)\n",
    "\n",
    "        next_token_logits = next_token_logits1 * (1/ temperature)\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_tokens = torch.multinomial(probs, num_samples=n_sample).squeeze(1)\n",
    "        #unfinished_sents = torch.ones(1, dtype=torch.long, device=device)\n",
    "        unfinished_sents = torch.ones(1, dtype=torch.long).cuda()\n",
    "        tokens_to_add = next_tokens * unfinished_sents + tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "\n",
    "        temp = []\n",
    "        for i in range(len(input_ids1)):\n",
    "            temp +=[torch.cat([input_ids1[i].reshape(1,-1), token_to_add.reshape(1,-1)], dim=-1) for token_to_add in tokens_to_add[i]]\n",
    "        input_ids1 = torch.stack(temp).view(len(temp),-1)\n",
    "        # decode the generated token ids to natural words\n",
    "        results = []\n",
    "        input_ids1_l = []\n",
    "        for input_id1 in input_ids1:\n",
    "            gen = tokenizer.decode(input_id1).replace(original,'').strip(' ')\n",
    "            if len(gen.split()) >0:\n",
    "                gen = gen.split()[0]\n",
    "                gen = gen.lower()\n",
    "                if gen not in results:\n",
    "                    results.append(gen)\n",
    "        return results\n",
    "        '''\n",
    "        if tokenizer.decode(tokens_to_add[0])[0] == ' ':\n",
    "            if current_word ==1:\n",
    "                return tokenizer.decode(input_ids1[0]).split()[-1], False\n",
    "            current_word += 1\n",
    "        input_ids1 = torch.cat([input_ids1, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76c5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69535c0b36444ff8a96f49619a700222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad3032bda554f4db2e204196c4d6166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6d6b96507b423580ae12cbfa6eadef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9997ff4ff986433096e11af7ca3310bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f708ae060143738d29d2c2a017ef1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58fadf601ad4bb7b445f5c9212cad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "gpt2_tokenizer  = AutoTokenizer.from_pretrained('gpt2-large')\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d0036",
   "metadata": {},
   "source": [
    "Update 1201: \n",
    "- dealing with a phrase\n",
    "- improvements of keywords enforcement\n",
    "- sample from the final beam\n",
    "\n",
    "made changes to the below functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc9a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularBeamSearch(prompts):\n",
    "\t'''\n",
    "\tBeam search that considers the coherence by adding a new variable: previously_generated_lines\n",
    "\t'''\n",
    "\tBeamScorer = {}\n",
    "\tfor sentence in prompts:\n",
    "\t\tloss = score_gpt2(sentence)\n",
    "\t\tBeamScorer[sentence] = [loss]\n",
    "\tanswers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "\tnew_prompts = [ans[0] for ans in answers]\n",
    "\treturn new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f20155",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "def sample_prompts(prompts,previous='', temperature = 1):\n",
    "    BeamScorer = {}\n",
    "    for sentence in prompts:\n",
    "        loss = score_gpt2(previous+sentence)\n",
    "        BeamScorer[sentence] = [loss]\n",
    "    p = BeamScorer.values()\n",
    "    p = torch.tensor(list(p))*(1/temperature)\n",
    "    try:\n",
    "        p = p.squeeze(1)\n",
    "    except:\n",
    "        pass\n",
    "    p_softmax = torch.nn.functional.softmax(p)\n",
    "    index = torch.multinomial(p_softmax,num_samples=len(prompts))\n",
    "    new_prompts = [prompts[i] for i in index]\n",
    "    return new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f6bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gpt2(sentence, normalize = True):\n",
    "\t'''\n",
    "\tThe default setting is to normalize because we won't face the issue mentioned in function \"score\".\n",
    "\t'''\n",
    "\ttokens_tensor = gpt2_tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = gpt2_model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019a1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myBeamSearch(prompts, all_states, all_n_sys, all_keywords, beam_size = 5,enforce_keywords=True):\n",
    "    BeamScorer = {}\n",
    "    return_seq, return_stt, return_sys, return_key = [], [], [], []\n",
    "    \n",
    "    if (not enforce_keywords) or len(all_keywords)==0:\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            loss = score(sentence)\n",
    "            BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "    else:\n",
    "        min_remaining = min([len(x) for x in all_keywords])\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #start with fewer keywords remaining\n",
    "            if len(keywords) == min_remaining:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers = sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "        BeamScorer={}\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #then\n",
    "            if len(keywords) == min_remaining+1:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers += sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "        BeamScorer={}\n",
    "        for sentence, p_state, n_sys, keywords in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "            #last, most keywords remaining\n",
    "            if len(keywords) == min_remaining+2:\n",
    "                loss = score(sentence)\n",
    "                BeamScorer[sentence] = [loss, p_state, n_sys, keywords]\n",
    "        answers += sorted(BeamScorer.items(), key=lambda x: x[1], reverse=False)\n",
    "    new_prompts = [ans[0] for ans in answers]\n",
    "    new_p_states = [ans[1][1] for ans in answers]\n",
    "    new_n_sys = [ans[1][2] for ans in answers]\n",
    "    new_keywords = [ans[1][3] for ans in answers]\n",
    "    l = len(new_prompts)\n",
    "    if l > beam_size:\n",
    "        return_seq += new_prompts[0:beam_size]\n",
    "        return_stt += new_p_states[0:beam_size]\n",
    "        return_sys += new_n_sys[0:beam_size]\n",
    "        return_key += new_keywords[0:beam_size]\n",
    "    else:\n",
    "        return_seq +=new_prompts\n",
    "        return_stt += new_p_states\n",
    "        return_sys += new_n_sys\n",
    "        return_key += new_keywords\n",
    "    return return_seq,return_stt, return_sys, return_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1144e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "562ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(sentence, normalize = True):\n",
    "\t'''\n",
    "\tScore a single sentence using the plan-to-lyrics model.\n",
    "\tThe recommended setting is to NOT normalize, because the input sentence is very long: it contains the title, planed keywords, and previously generated lines. \n",
    "\tIn addition, the candidate sentences contain the same prefix (i.e., the title, planed keywords, and previously generated lines) and only differ in the currently generated line.\n",
    "\tNormaling means dividing the loss by a large factor which may result in similarity accross different candidate sentences.\n",
    "\t'''\n",
    "\ttokens_tensor = tokenizer.encode(sentence, add_special_tokens=False, return_tensors=\"pt\")[0].cuda()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss = model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "\tif normalize:\n",
    "\t\treturn loss/len(tokens_tensor)\n",
    "\telse:\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c32c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stress_phrase(phrase):\n",
    "    words = phrase.split()\n",
    "    stress=[]\n",
    "    for source_word in words:\n",
    "        phone = pronouncing.phones_for_word(source_word)[0]\n",
    "        stress+= get_stress(phone)\n",
    "    return stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090524aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_character_word = ['i','a']\n",
    "forbidden_words = ['dona','er','ira','ia',\"'s\",\"'m\",\"hmm\",\"mm\"]\n",
    "def get_valid_samples(prompt, p_state, n_syllables, keywords, n_sample=30, n_cands=5):\n",
    "    #if n_syllables == 10 or n_syllables==11:\n",
    "    if n_syllables == 10 and len(keywords)==0:\n",
    "        return [prompt], [p_state], [n_syllables], [keywords]\n",
    "    elif n_syllables > 10:\n",
    "        return [], [], [],[]\n",
    "    states = []\n",
    "    all_n_syl = []\n",
    "    \n",
    "    prompts = []\n",
    "    all_keywords= [] \n",
    "    #insert the keyword whenever possible\n",
    "    for source_word in keywords:\n",
    "        stress = get_stress_phrase(source_word)\n",
    "        #if not alternating(stress):\n",
    "            #continue\n",
    "\n",
    "        #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "        flag = check_either_stress(stress, source_word)\n",
    "\n",
    "        if (stress[-1] == 1- p_state or flag) and (n_syllables+len(stress)<=10):\n",
    "            states.append(stress[0])\n",
    "            all_n_syl.append(n_syllables+len(stress))\n",
    "            #print(source_word)\n",
    "            prompts.append(prompt+ ' ' +reverse_order(source_word))\n",
    "            copy = keywords.copy()\n",
    "            copy.remove(source_word)\n",
    "            all_keywords.append(copy)    \n",
    "    \n",
    "    #The normal process of decoding\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    tokens = generate_next_word(input_ids, n_sample=n_sample)\n",
    "    #print(tokens)\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if (len(token) == 1 and token not in single_character_word) or token in forbidden_words:\n",
    "            continue\n",
    "        if token not in prompt:\n",
    "            try:\n",
    "                phone = pronouncing.phones_for_word(token)[0]\n",
    "                stress = get_stress(phone)\n",
    "            except:\n",
    "                continue\n",
    "            if (not alternating(stress)) or (len(stress)==0):\n",
    "                continue\n",
    "\n",
    "            #if the word is single syllable and can be either stressed or unstressed, flag = True\n",
    "            flag = check_either_stress(stress, token)\n",
    "            if n_syllables+len(stress)<=10:\n",
    "                if (stress[-1] == 1- p_state) or flag:\n",
    "                    tokens.append(token)\n",
    "                    if stress[-1] == 1- p_state:\n",
    "                        states.append(stress[0])\n",
    "                    elif flag:\n",
    "                        states.append(1- p_state)\n",
    "                    all_n_syl.append(n_syllables+len(stress))\n",
    "                    prompts.append(prompt+ ' ' + token )\n",
    "                    all_keywords.append(keywords)\n",
    "                    if len(prompts)>= n_cands:\n",
    "                        return prompts, states, all_n_syl, all_keywords\n",
    "    return prompts, states, all_n_syl, all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0b258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9200640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['susie', 'nature', 'loved']\n",
      "['that was the nature susie known and loved', 'with plants and all the nature susie loved', 'that is the nature susie known and loved', 'as plants and all the nature susie loved', 'as is the nature susie known and loved', 'and how the cruel nature susie loved', 'the wild and cruel nature susie loved', 'the harsh and cruel nature susie loved', 'that world and all the nature susie loved', 'is how the cruel nature susie loved', 'and beauty that the nature susie loved', 'that was the very nature susie loved', 'like how the cruel nature susie loved', 'that is the very nature susie loved', 'and all that cruel nature susie loved', 'with all that cruel nature susie loved', 'that harsh and cruel nature susie loved', 'and was the very nature susie loved', 'and silence that the nature susie loved']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███▏                                        | 1/14 [00:27<06:02, 27.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'to', 'wanted']\n",
      "['let go to as she is always wanted', 'would go to as she is always wanted', 'not go to as she is always wanted', 'her go to as she is always wanted', 'much as to that she is always wanted', 'all go to as she is always wanted', 'let go to that she is always wanted', 'with care as to she is always wanted', 'care as to that she is always wanted', 'close as to that she is always wanted', 'with her as to she is always wanted', 'did go to that she is always wanted', 'life as to that she is always wanted', 'her go to that she is always wanted', 'you go to that she is always wanted', 'will go to that she is always wanted', 'man as to that she is always wanted', 'that world as to she is always wanted', 'able to as she is always wanted', 'going to as she is always wanted']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████▎                                     | 2/14 [01:06<06:53, 34.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beach', 'her', 'beloved']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|█████████▍                                  | 3/14 [01:45<06:41, 36.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hers was that lonely beach with her beloved', 'about that lonely beach with her beloved', 'towards that lonely beach with her beloved', 'that quiet little beach with her beloved', 'about that little beach with her beloved']\n",
      "['sunset', 'beautiful', 'undaunted']\n",
      "['the sunset beautiful and undaunted', 'the beautiful sunset still undaunted', 'the beautiful sunset lies undaunted', 'the sunset beautiful but undaunted', 'the beautiful sunset went undaunted', 'the beautiful sunset but undaunted', 'and beautiful sunset but undaunted', 'and beautiful sunset lies undaunted', 'and beautiful sunset went undaunted', 'and sunset beautiful but undaunted', 'the sunset beautiful lies undaunted', 'the sunset beautiful still undaunted', 'the sunset beautiful went undaunted']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▌                               | 4/14 [02:13<05:28, 32.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relaxing', 'day', 'spent']\n",
      "['as how the most relaxing day is spent', 'the joy as your relaxing day is spent', 'like how the slow relaxing day is spent', 'the calm as your relaxing day is spent', 'perhaps the most relaxing day is spent', 'the second most relaxing day is spent', 'the most relaxing quiet day is spent', 'as our most relaxing day is spent', 'the most relaxing day is being spent', 'enjoy the slow relaxing day is spent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████▋                            | 5/14 [02:52<05:16, 35.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nervous', 'were', 'both']\n",
      "['that people who are nervous were with both', 'like lovers who are nervous were with both', 'like people who are nervous were with both', 'that they are very nervous were with both', 'are very nervous those that were with both', 'that ladies who are nervous were with both', 'that women who are nervous were with both', 'that maybe they are nervous were with both', 'like getting nervous ones that were with both', 'are getting nervous ones that were with both']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████▊                         | 6/14 [03:36<05:06, 38.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['couple', 'talked', 'sent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 7/14 [04:07<04:10, 35.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and talked another couple being sent', 'another couple talked and also sent', 'like any couple talked already sent', 'and little couple talked already sent', 'and any couple talked already sent']\n",
      "['back', 'home', 'growth']\n",
      "['go back like home requires further growth', 'is back like home requires further growth', 'go back is home observing our growth', 'is home go back observing our growth', 'like our home go back consider growth', 'as our home go back consider growth']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████▏                  | 8/14 [04:39<03:27, 34.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quiet', 'afterwards', 'felt']\n",
      "['we are just as quiet afterwards felt', 'you are just as quiet afterwards felt', 'who are just as quiet afterwards felt', 'men are just as quiet afterwards felt', 'both are just as quiet afterwards felt', 'we are very quiet afterwards felt', 'we are really quiet afterwards felt', 'was that quiet only afterwards felt', 'we are quiet only afterwards felt', 'they are very quiet afterwards felt', 'was that quiet afterwards always felt', 'thoughts are quiet only afterwards felt', 'thoughts are very quiet afterwards felt', 'silence just as quiet afterwards felt', 'you are quiet only afterwards felt', 'kept that quiet afterwards always felt', 'was that quiet silence afterwards felt', 'you are really quiet afterwards felt', 'still that quiet only afterwards felt', 'still that quiet silence afterwards felt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████▎               | 9/14 [05:10<02:47, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone like a stream', 'glad', 'now']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████████████▋            | 10/14 [05:45<02:15, 33.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glad alone like a stream with silence now', 'glad alone like a stream the silence now', 'glad alone like a stream the waters now']\n",
      "['sleep', 'could', 'belt']\n",
      "['this is all that could go sleep and belt', 'do with all that could go sleep and belt', 'is any way that could go sleep and belt', 'enough with all that could go sleep and belt', 'comes with all that could go sleep and belt', 'best with all that could go sleep and belt', 'is doing all that could go sleep and belt', 'is many ways that could go sleep and belt', 'is only way that could go sleep and belt', 'left with all that could go sleep and belt', 'day is all that could go sleep and belt', 'with any way that could go sleep and belt', 'know is all that could go sleep and belt', 'do is all that could go sleep and belt', 'is only ways that could go sleep and belt', 'with many ways that could go sleep and belt', 'light is all that could go sleep and belt', 'is sit and sleep that could with our belt', 'with doing all that could go sleep and belt', 'with only ways that could go sleep and belt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████▊         | 11/14 [06:36<01:57, 39.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asleep', 'night', 'how']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████▊      | 12/14 [07:01<01:09, 34.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go into night asleep remember how', 'asleep another night imagine how']\n",
      "['sleeping', 'restless', 'feeling']\n",
      "['likely was the restless sleeping feeling', 'you ignore the restless sleeping feeling', 'well except the restless sleeping feeling', 'clearly was the restless sleeping feeling', 'reason for the restless sleeping feeling', 'we ignore the restless sleeping feeling', 'was the always restless sleeping feeling', 'really was the restless sleeping feeling', 'know except the restless sleeping feeling', 'just ignore the restless sleeping feeling', 'we are always restless sleeping feeling', 'second was the restless sleeping feeling', 'us are always restless sleeping feeling', 'from the restless sleeping troubled feeling', 'culprit for the restless sleeping feeling', 'you are always restless sleeping feeling', 'was the restless sleeping troubled feeling', 'you are really restless sleeping feeling', 'kids are always restless sleeping feeling', 'things except the restless sleeping feeling']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|███████████████████████████████████████▉   | 13/14 [07:33<00:34, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wake', 'morning', 'appealing']\n",
      "['but that morning wake is not appealing', 'why that morning wake is not appealing', 'and that morning wake is not appealing', 'now that morning wake is not appealing', 'well that morning wake is not appealing', 'there and morning wake is not appealing', 'noon and morning wake is not appealing', 'want and morning wake is not appealing', 'you and morning wake is not appealing', 'its and morning wake is not appealing', 'always wake that morning not appealing', 'never wake that morning not appealing', 'always wake that morning quite appealing', 'even wake that morning is appealing', 'never wake that morning quite appealing', 'even morning wake is not appealing', 'little morning wake is not appealing', 'even wake that morning not appealing', 'truly wake that morning is appealing', 'other wake that morning is appealing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 14/14 [08:07<00:00, 34.80s/it]\n"
     ]
    }
   ],
   "source": [
    "four_seasons_story_line = [\n",
    "['susie', 'nature', 'loved'],\n",
    "['she', 'to', 'wanted'],\n",
    "['beach', 'her', 'beloved'],\n",
    "['sunset', 'beautiful', 'undaunted'], \n",
    "['relaxing', 'day', 'spent'],\n",
    "['nervous', 'were', 'both'],\n",
    "['couple', 'talked', 'sent'],\n",
    "['back', 'home', 'growth'],\n",
    "['quiet', 'afterwards', 'felt'],\n",
    "['alone like a stream', 'glad', 'now'],\n",
    "['sleep', 'could', 'belt'],\n",
    "['asleep', 'night', 'how'],\n",
    "['sleeping', 'restless', 'feeling'],\n",
    "['wake', 'morning', 'appealing']]\n",
    "\n",
    "\n",
    "example_title = 'solitary contemplations in nature'\n",
    "beam_size=20\n",
    "previous = \"\"\n",
    "enforce_keywords = True\n",
    "for kws in tqdm(four_seasons_story_line):\n",
    "    success=False\n",
    "    n_sample = 30\n",
    "    while success != True:\n",
    "        print(kws)\n",
    "        rhyme_word = kws[-1]\n",
    "        prefix =  '''Keywords: ''' + '; '.join(kws) +'. Sentence in reverse order: '\n",
    "        prompt = '''<|startoftext|> Title: ''' + example_title + ' ' + ','.join(previous.split(',')[-3:]) + prefix + rhyme_word\n",
    "        #prompt = '''<|startoftext|> Title: ''' + example_title + ' ' + prefix + rhyme_word\n",
    "        p_state, n_syllables = get_phones(rhyme_word)\n",
    "        result_list = []\n",
    "        i=0\n",
    "        prompts, all_states, all_n_sys, all_keywords = get_valid_samples(prompt,p_state, n_syllables, keywords = kws[:2], n_sample=n_sample,n_cands=5)\n",
    "        while i<7:\n",
    "            #print(i)\n",
    "            new_prompts, new_states, new_n_sys, new_keywords = [], [], [], []\n",
    "            for prompt, p_state, n_syllables, keyword in zip(prompts, all_states, all_n_sys, all_keywords):\n",
    "                t_p, t_state, t_sys, t_keywords = get_valid_samples(prompt, p_state, n_syllables, keyword,n_sample=n_sample)\n",
    "                new_prompts+=t_p\n",
    "                new_states+=t_state\n",
    "                new_n_sys+=t_sys\n",
    "                new_keywords+=t_keywords\n",
    "            prompts, all_states, all_n_sys, all_keywords = new_prompts, new_states, new_n_sys, new_keywords\n",
    "            prompts, all_states, all_n_sys, all_keywords = myBeamSearch(prompts,all_states, all_n_sys, all_keywords, beam_size=beam_size, enforce_keywords=enforce_keywords)\n",
    "            i += 1\n",
    "        if len(prompts)==0:\n",
    "            if n_sample>300:\n",
    "                print('Failed to generate valid samples. Please try re-generation for this line.')\n",
    "                previous += '   ,'\n",
    "                break\n",
    "            n_sample = n_sample*3\n",
    "\n",
    "        else:\n",
    "            correct_prompts = [reverse_order(p.split('order: ')[1]) for p in prompts]\n",
    "            print(correct_prompts)\n",
    "            result_list = sample_prompts(correct_prompts, previous)\n",
    "            \n",
    "            success=True\n",
    "            found = False \n",
    "            for r in result_list:\n",
    "                if kws[0] in r or kws[1] in r:\n",
    "                    previous = previous + r + ','\n",
    "                    found = True\n",
    "                    break\n",
    "            if found == False:\n",
    "                    previous = previous + result_list[0]+','\n",
    "                    n_sample = n_sample*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ef9703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce keywords:\n",
      "\n",
      "the wild and cruel nature susie loved,\n",
      "not go to as she is always wanted,\n",
      "towards that lonely beach with her beloved,\n",
      "and beautiful sunset went undaunted,\n",
      "the joy as your relaxing day is spent,\n",
      "that people who are nervous were with both,\n",
      "another couple talked and also sent,\n",
      "go back is home observing our growth,\n",
      "we are just as quiet afterwards felt,\n",
      "glad alone like a stream the silence now,\n",
      "is sit and sleep that could with our belt,\n",
      "go into night asleep remember how,\n",
      "just ignore the restless sleeping feeling,\n",
      "other wake that morning is appealing,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Enforce keywords:\\n')\n",
    "\n",
    "print(previous.replace(',',',\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e9344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
